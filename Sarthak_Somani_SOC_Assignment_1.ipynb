{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMxNKWiDZfQS5ZLWlKN1fkf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarthak-somani/SOC-2025-Morphix/blob/main/Sarthak_Somani_SOC_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_kfskWRK9FvK",
        "outputId": "c578ee3a-0688-42d1-ea7c-dd28b09ebfcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'stylegan2-ada-pytorch'...\n",
            "remote: Enumerating objects: 131, done.\u001b[K\n",
            "remote: Counting objects: 100% (2/2), done.\u001b[K\n",
            "remote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 131 (delta 0), reused 0 (delta 0), pack-reused 129 (from 2)\u001b[K\n",
            "Receiving objects: 100% (131/131), 1.13 MiB | 38.59 MiB/s, done.\n",
            "Resolving deltas: 100% (57/57), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd stylegan2-ada-pytorch"
      ],
      "metadata": {
        "id": "NXlhdkbCDVG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install click requests tqdm pyspng ninja imageio-ffmpeg==0.4.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1474528-8281-48a5-a9d9-7f57c491c385",
        "id": "AWLc92cEDbC9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (8.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Collecting pyspng\n",
            "  Downloading pyspng-0.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n",
            "Collecting imageio-ffmpeg==0.4.3\n",
            "  Downloading imageio_ffmpeg-0.4.3-py3-none-manylinux2010_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.6.15)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pyspng) (2.0.2)\n",
            "Downloading imageio_ffmpeg-0.4.3-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyspng-0.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (196 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.1/196.1 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspng, ninja, imageio-ffmpeg\n",
            "  Attempting uninstall: imageio-ffmpeg\n",
            "    Found existing installation: imageio-ffmpeg 0.6.0\n",
            "    Uninstalling imageio-ffmpeg-0.6.0:\n",
            "      Successfully uninstalled imageio-ffmpeg-0.6.0\n",
            "Successfully installed imageio-ffmpeg-0.4.3 ninja-1.11.1.4 pyspng-0.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7YE_b4Z-DbzZ",
        "outputId": "918eb583-1739-4bef-8d0d-f9a443a55c68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-25 17:15:53--  https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl\n",
            "Resolving nvlabs-fi-cdn.nvidia.com (nvlabs-fi-cdn.nvidia.com)... 13.35.37.10, 13.35.37.106, 13.35.37.115, ...\n",
            "Connecting to nvlabs-fi-cdn.nvidia.com (nvlabs-fi-cdn.nvidia.com)|13.35.37.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 381624121 (364M) [binary/octet-stream]\n",
            "Saving to: ‘ffhq.pkl’\n",
            "\n",
            "ffhq.pkl            100%[===================>] 363.94M   195MB/s    in 1.9s    \n",
            "\n",
            "2025-06-25 17:15:55 (195 MB/s) - ‘ffhq.pkl’ saved [381624121/381624121]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/stylegan2-ada-pytorch/generate.py --outdir=out --trunc=1 --seeds=2,12,42 --network=ffhq.pkl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wgz5DOpRD7M_",
        "outputId": "858c41a8-9462-4fc1-9ec2-b2da417c04dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading networks from \"ffhq.pkl\"...\n",
            "Generating image for seed 2 (0/3) ...\n",
            "Setting up PyTorch plugin \"bias_act_plugin\"... /usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "Done.\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... /usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:2059: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "Done.\n",
            "Generating image for seed 12 (1/3) ...\n",
            "Generating image for seed 42 (2/3) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# --- Step 1: Set up the environment and device ---\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# --- Step 2: Load the Pre-trained StyleGAN2-ADA Model ---\n",
        "model_path = '../ffhq.pkl'\n",
        "print(f'Loading networks from \"{model_path}\"...')\n",
        "\n",
        "# Check if the model file exists before attempting to load it.\n",
        "if not os.path.exists(model_path):\n",
        "    print(f\"Error: Model file not found at '{model_path}'.\")\n",
        "    print(\"Please ensure the path is correct and you have downloaded the model.\")\n",
        "else:\n",
        "    # Load the generator network from the pickle file.\n",
        "    # We load it into memory and then move the model to the selected device (GPU/CPU).\n",
        "    # The 'G_ema' key holds the Exponential Moving Average of the generator's weights\n",
        "    with open(model_path, 'rb') as f:\n",
        "        G = pickle.load(f)['G_ema'].to(device)\n",
        "    print('Model loaded successfully.')\n",
        "\n",
        "    # Set the model to evaluation mode. This disables layers like dropout.\n",
        "    G.eval()\n",
        "\n",
        "    # --- Step 3: Generate a random Z vector ---\n",
        "\n",
        "    # G.z_dim is the dimension of the Z space for the loaded model (typically 512).\n",
        "    # The shape will be [1, 512], where 1 is the batch size.\n",
        "    z_latent = np.random.randn(1, G.z_dim)\n",
        "\n",
        "    # Convert the NumPy array to a PyTorch tensor and move it to the configured device.\n",
        "    z_tensor = torch.from_numpy(z_latent).float().to(device)\n",
        "\n",
        "    # --- Step 4: Map Z to W+ Space ---\n",
        "    # We pass the Z tensor through the generator's mapping network.\n",
        "    # This model's mapping network directly produces the W+ vector, which contains\n",
        "    # a separate W vector for each layer of the synthesis network.\n",
        "    print('Mapping Z vector directly to W+ space...')\n",
        "    with torch.no_grad():\n",
        "        # The resulting shape should be [1, num_layers, 512], e.g., [1, 18, 512].\n",
        "        w_plus_latent = G.mapping(z_tensor, None)\n",
        "\n",
        "    # --- Step 5: Derive W from W+ for Saving ---\n",
        "    # The \"conceptual\" W space vector ([1, 512]) can be derived from W+.\n",
        "    # For simplicity, we'll take the style vector for the first layer.\n",
        "    # The previous logic that repeated the vector was incorrect for this model and has been removed.\n",
        "    w_latent = w_plus_latent[:, 0, :]\n",
        "\n",
        "    # --- Verification and Summary ---\n",
        "    # Check the shapes to confirm they match the desired dimensions.\n",
        "    print(\"\\n--- Corrected Latent Vector Shapes ---\")\n",
        "    print(f\"Z  (initial latent):  {z_tensor.shape}\")\n",
        "    print(f\"W  (derived latent):   {w_latent.shape}\")\n",
        "    print(f\"W+ (direct from map): {w_plus_latent.shape}\")\n",
        "    print(\"--------------------------------------\")\n",
        "\n",
        "    # --- Step 6: Save the W and W+ vectors ---\n",
        "    # These .npy files can be used by other scripts for image generation or style mixing.\n",
        "    w_numpy = w_latent.cpu().numpy()\n",
        "    np.save('latent_w_01.npy', w_numpy)\n",
        "\n",
        "    w_plus_numpy = w_plus_latent.cpu().numpy()\n",
        "    np.save('latent_w_plus_01.npy', w_plus_numpy)\n",
        "\n",
        "    print(\"\\nSuccessfully saved 'latent_w_01.npy' and 'latent_w_plus_01.npy'.\")\n",
        "    print(\"These files are now ready for use in a generation script.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTCXtmQoN6iM",
        "outputId": "8de91f21-59bb-466b-82a9-6e223b27cf67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading networks from \"../ffhq.pkl\"...\n",
            "Model loaded successfully.\n",
            "Mapping Z vector directly to W+ space...\n",
            "\n",
            "--- Corrected Latent Vector Shapes ---\n",
            "Z  (initial latent):  torch.Size([1, 512])\n",
            "W  (derived latent):   torch.Size([1, 512])\n",
            "W+ (direct from map): torch.Size([1, 18, 512])\n",
            "--------------------------------------\n",
            "\n",
            "Successfully saved 'latent_w_01.npy' and 'latent_w_plus_01.npy'.\n",
            "These files are now ready for use in a generation script.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import pickle\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# --- Step 1: Set up the environment and device ---\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# --- Step 2: Load the Pre-trained StyleGAN2-ADA Model ---\n",
        "model_path = '../ffhq.pkl'\n",
        "print(f'Loading networks from \"{model_path}\"...')\n",
        "\n",
        "# Check if the model file exists before proceeding\n",
        "if not os.path.exists(model_path):\n",
        "    print(\"Error: Model file not found. Make sure 'ffhq.pkl' is in the parent directory.\")\n",
        "else:\n",
        "    with open(model_path, 'rb') as f:\n",
        "        # Load the generator network and move it to the correct device\n",
        "        G = pickle.load(f)['G_ema'].to(device)\n",
        "    print('Model loaded successfully.')\n",
        "    G.eval()\n",
        "\n",
        "    # --- Step 3: Define a helper function to convert tensor to image ---\n",
        "    def tensor_to_pil(tensor):\n",
        "        \"\"\"\n",
        "        Converts a PyTorch tensor (in NCHW format) to a PIL Image.\n",
        "        The tensor is expected to be in the range [-1, 1].\n",
        "        \"\"\"\n",
        "        # Denormalize from [-1, 1] to [0, 255]\n",
        "        tensor = (tensor.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "        # Convert to a PIL image from the first item in the batch\n",
        "        return Image.fromarray(tensor[0].cpu().numpy(), 'RGB')\n",
        "\n",
        "    # --- File paths for your saved latent vectors ---\n",
        "    w_vector_path = 'latent_w_01.npy'\n",
        "    w_plus_vector_path = 'latent_w_plus_01.npy'\n",
        "\n",
        "    # --- Check if the latent vector files exist before trying to load them ---\n",
        "    if not os.path.exists(w_vector_path) or not os.path.exists(w_plus_vector_path):\n",
        "         print(f\"Error: Latent vector files not found. Please create them first.\")\n",
        "    else:\n",
        "        # --- Method A: Generate Image from the saved W vector ---\n",
        "        print(\"\\nGenerating from the standard W vector...\")\n",
        "        w_numpy = np.load(w_vector_path)\n",
        "        w_tensor = torch.from_numpy(w_numpy).to(device)\n",
        "\n",
        "        # The synthesis network ALWAYS expects a 3D W+ tensor of shape [batch, num_ws, w_dim].\n",
        "        # We must prepare the tensor accordingly.\n",
        "        # This is a correctly-shaped W vector, e.g., [1, 512], so we expand it.\n",
        "        print(f\"  Loaded W vector has shape: {w_tensor.shape}\")\n",
        "        w_for_synthesis = w_tensor.unsqueeze(1).repeat([1, G.num_ws, 1])\n",
        "        print(f\"  Expanded to W+ shape for synthesis: {w_for_synthesis.shape}\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            img_from_w = G.synthesis(w_for_synthesis, noise_mode='const')\n",
        "\n",
        "        pil_img_from_w = tensor_to_pil(img_from_w)\n",
        "        pil_img_from_w.save('generated_from_W.png')\n",
        "        print(f\"-> Successfully saved 'generated_from_W.png'.\")\n",
        "\n",
        "        # --- Method B: Generate Image from the saved W+ vector ---\n",
        "        print(\"\\nGenerating from the W+ vector...\")\n",
        "        w_plus_numpy = np.load(w_plus_vector_path)\n",
        "        w_plus_tensor = torch.from_numpy(w_plus_numpy).to(device)\n",
        "        print(f\"  Loaded W+ vector has correct shape: {w_plus_tensor.shape}\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            img_from_w_plus = G.synthesis(w_plus_tensor, noise_mode='const')\n",
        "\n",
        "        pil_img_from_w_plus = tensor_to_pil(img_from_w_plus)\n",
        "        pil_img_from_w_plus.save('generated_from_W_plus.png')\n",
        "        print(f\"-> Successfully saved 'generated_from_W_plus.png'.\")\n",
        "\n",
        "        # The two generated images should now be identical.\n",
        "        print(\"\\nGeneration complete. The two images, 'generated_from_W.png' and 'generated_from_W_plus.png', should look the same.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRfCHgq7PsGt",
        "outputId": "b80aea97-8edf-41ec-bc59-c7b51e7c9768"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading networks from \"../ffhq.pkl\"...\n",
            "Model loaded successfully.\n",
            "\n",
            "Generating from the standard W vector...\n",
            "  Loaded W vector has shape: torch.Size([1, 512])\n",
            "  Expanded to W+ shape for synthesis: torch.Size([1, 18, 512])\n",
            "-> Successfully saved 'generated_from_W.png'.\n",
            "\n",
            "Generating from the W+ vector...\n",
            "  Loaded W+ vector has correct shape: torch.Size([1, 18, 512])\n",
            "-> Successfully saved 'generated_from_W_plus.png'.\n",
            "\n",
            "Generation complete. The two images, 'generated_from_W.png' and 'generated_from_W_plus.png', should look the same.\n"
          ]
        }
      ]
    }
  ]
}