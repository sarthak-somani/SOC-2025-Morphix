{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarthak-somani/SOC-2025-Morphix/blob/main/Sarthak_Somani_SOC_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egJgo4Er526N"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfF4XTtX6F62"
      },
      "outputs": [],
      "source": [
        "%cd stylegan2-ada-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbgLXpx56JvI"
      },
      "outputs": [],
      "source": [
        "!pip install click requests tqdm pyspng ninja imageio-ffmpeg==0.4.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrqdtdG26NK8"
      },
      "outputs": [],
      "source": [
        "!wget https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OiGqY8ut6RqC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_KxG1MA6i4R"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ILL_YNbY6mcz"
      },
      "outputs": [],
      "source": [
        "model_path = 'ffhq.pkl'\n",
        "print(f'Loading networks from \"{model_path}\"...')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vp4wu__l6ouQ"
      },
      "outputs": [],
      "source": [
        "%cd /content/stylegan2-ada-pytorch\n",
        "with open(model_path, 'rb') as f:\n",
        "  G = pickle.load(f)['G_ema'].to(device)\n",
        "  print('Model loaded successfully.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vdc1ykmu7c-5"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "def tensor_to_pil(tensor):\n",
        "        \"\"\"\n",
        "        Converts a PyTorch tensor (in NCHW format) to a PIL Image.\n",
        "        The tensor is expected to be in the range [-1, 1].\n",
        "        \"\"\"\n",
        "        # Denormalize from [-1, 1] to [0, 255]\n",
        "        tensor = (tensor.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "        # Convert to a PIL image from the first item in the batch\n",
        "        return Image.fromarray(tensor[0].cpu().numpy(), 'RGB')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z95hHKox5KX_"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "def display_two_images(image1, image2, title1=\"Image 1\", title2=\"Image 2\"):\n",
        "  \"\"\"\n",
        "  Displays two images side-by-side using matplotlib.\n",
        "\n",
        "  Args:\n",
        "    image1: The first image to display.\n",
        "    image2: The second image to display.\n",
        "    title1 (str): An optional title for the first image.\n",
        "    title2 (str): An optional title for the second image.\n",
        "  \"\"\"\n",
        "  # Create a figure with 1 row and 2 columns for the subplots.\n",
        "  fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "  # Display the first image on the left subplot (axes[0])\n",
        "  axes[0].imshow(image1)\n",
        "  axes[0].set_title(title1)\n",
        "  axes[0].axis('off') # Hide the axes ticks and labels\n",
        "\n",
        "  # Display the second image on the right subplot (axes[1])\n",
        "  axes[1].imshow(image2)\n",
        "  axes[1].set_title(title2)\n",
        "  axes[1].axis('off') # Hide the axes ticks and labels\n",
        "\n",
        "  plt.tight_layout() # Adjust layout to prevent titles from overlapping\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFBD5Uzh7Nu4"
      },
      "outputs": [],
      "source": [
        "z_latent = np.random.randn(1, G.z_dim)\n",
        "z_tensor = torch.from_numpy(z_latent).float().to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "  w_latent = G.mapping(z_tensor, None)\n",
        "  img_from_w = G.synthesis(w_latent, noise_mode='const')\n",
        "\n",
        "pil_img_from_w = tensor_to_pil(img_from_w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dOdcebyYx89H"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i12rJSKpAfZS"
      },
      "outputs": [],
      "source": [
        "%cd /content/drive/MyDrive/IIT Bombay /SS - WnCC (Personal)/StyleGAN 2 Boundries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hf4UVx7v-0QW"
      },
      "outputs": [],
      "source": [
        "smile_w = np.load('smile.npy')\n",
        "age_w = np.load('age.npy')\n",
        "gender_w = np.load('gender.npy')\n",
        "\n",
        "\n",
        "smile_w_tensor = torch.from_numpy(smile_w).to(device).float()\n",
        "gender_w_tensor = torch.from_numpy(gender_w).to(device).float()\n",
        "age_w_tensor = torch.from_numpy(age_w).to(device).float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwkiPSLJU3AJ"
      },
      "outputs": [],
      "source": [
        "w_latent_new = w_latent + age_w_tensor*3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfPCmuibVZN1"
      },
      "outputs": [],
      "source": [
        "img_from_w_latent_new = G.synthesis(w_latent_new, noise_mode='const')\n",
        "pil_img_from_w_latent_new = tensor_to_pil(img_from_w_latent_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Y6Tfevg0Xgs"
      },
      "outputs": [],
      "source": [
        "display_two_images(pil_img_from_w, pil_img_from_w_latent_new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cF7E6NC_7QiD"
      },
      "outputs": [],
      "source": [
        "!pip install -q streamlit\n",
        "!npm install -g localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDJAw09j7__Z"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "# app.py\n",
        "import streamlit as st\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import os\n",
        "import requests\n",
        "import subprocess\n",
        "import pickle\n",
        "import torch\n",
        "import sys  # <--- ADD THIS IMPORT\n",
        "\n",
        "# --- Constants ---\n",
        "STYLEGAN_REPO_DIR = \"stylegan2-ada-pytorch\"\n",
        "# Add the StyleGAN repo to Python's path to find the custom 'torch_utils' module\n",
        "sys.path.append(STYLEGAN_REPO_DIR)  # <--- ADD THIS LINE\n",
        "\n",
        "MODEL_URL = \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl\"\n",
        "MODEL_PATH = os.path.join(STYLEGAN_REPO_DIR, \"ffhq.pkl\")\n",
        "MORPHIX_REPO_DIR = \"SOC-2025-Morphix\"\n",
        "\n",
        "# --- Page Configuration ---\n",
        "st.set_page_config(\n",
        "    page_title=\"Latent Editor UI\",\n",
        "    page_icon=\"ðŸŽ¨\",\n",
        "    layout=\"wide\",\n",
        ")\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def tensor_to_pil(tensor):\n",
        "    \"\"\"Converts a PyTorch tensor to a PIL Image.\"\"\"\n",
        "    tensor = (tensor.permute(0, 2, 3, 1) * 127.5 + 128).clamp(0, 255).to(torch.uint8)\n",
        "    return Image.fromarray(tensor[0].cpu().numpy(), 'RGB')\n",
        "\n",
        "def image_to_bytes(img):\n",
        "    \"\"\"Converts a PIL Image to bytes for downloading.\"\"\"\n",
        "    buf = BytesIO()\n",
        "    img.save(buf, format=\"PNG\")\n",
        "    return buf.getvalue()\n",
        "\n",
        "# --- Backend Loading ---\n",
        "@st.cache_resource\n",
        "def load_backend():\n",
        "    \"\"\"\n",
        "    Clones repositories, downloads the model, and loads all assets into memory.\n",
        "    This function runs only once and its return value is cached.\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # 1. Clone StyleGAN repo if it doesn't exist\n",
        "    if not os.path.exists(STYLEGAN_REPO_DIR):\n",
        "        st.info(\"Cloning StyleGAN2-ADA repository...\")\n",
        "        subprocess.run([\"git\", \"clone\", \"https://github.com/NVlabs/stylegan2-ada-pytorch.git\", STYLEGAN_REPO_DIR], check=True)\n",
        "\n",
        "    # 2. Download StyleGAN model if it doesn't exist\n",
        "    if not os.path.exists(MODEL_PATH):\n",
        "        st.info(\"Downloading StyleGAN model (ffhq.pkl)...\")\n",
        "        with requests.get(MODEL_URL, stream=True) as r:\n",
        "            r.raise_for_status()\n",
        "            with open(MODEL_PATH, 'wb') as f:\n",
        "                for chunk in r.iter_content(chunk_size=8192):\n",
        "                    f.write(chunk)\n",
        "\n",
        "    # 3. Load the StyleGAN model\n",
        "    with open(MODEL_PATH, 'rb') as f:\n",
        "        G = pickle.load(f)['G_ema'].to(device)\n",
        "    st.success(\"âœ… StyleGAN model loaded successfully!\")\n",
        "\n",
        "    # 4. Clone the Morphix repo for latent vectors\n",
        "    if not os.path.exists(MORPHIX_REPO_DIR):\n",
        "        st.info(\"Cloning Morphix repository for latent vectors...\")\n",
        "        subprocess.run([\"git\", \"clone\", \"https://github.com/sarthak-somani/SOC-2025-Morphix.git\", MORPHIX_REPO_DIR], check=True)\n",
        "\n",
        "    # 5. Load latent direction vectors\n",
        "    try:\n",
        "        smile_w = np.load(os.path.join(MORPHIX_REPO_DIR, 'Models', 'smile.npy'))\n",
        "        age_w = np.load(os.path.join(MORPHIX_REPO_DIR, 'Models', 'age.npy'))\n",
        "        gender_w = np.load(os.path.join(MORPHIX_REPO_DIR, 'Models', 'gender.npy'))\n",
        "        st.success(\"âœ… Latent vectors loaded successfully!\")\n",
        "    except FileNotFoundError as e:\n",
        "        st.error(f\"Error loading latent vectors: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Return all loaded assets in a dictionary\n",
        "    return {\n",
        "        \"G\": G,\n",
        "        \"device\": device,\n",
        "        \"age_w\": torch.from_numpy(age_w).to(device),\n",
        "        \"smile_w\": torch.from_numpy(smile_w).to(device),\n",
        "        \"gender_w\": torch.from_numpy(gender_w).to(device),\n",
        "    }\n",
        "\n",
        "# --- Load assets and store them in session state ---\n",
        "if 'backend_assets' not in st.session_state:\n",
        "    with st.spinner(\"ðŸš€ Starting up... Loading models and assets...\"):\n",
        "        st.session_state.backend_assets = load_backend()\n",
        "\n",
        "# --- Image Generation ---\n",
        "def generate_image():\n",
        "    \"\"\"Generates an image based on current session state values.\"\"\"\n",
        "    assets = st.session_state.backend_assets\n",
        "    if not assets:\n",
        "        st.error(\"Backend assets not loaded. Cannot generate image.\")\n",
        "        return Image.new('RGB', (256, 256), color = 'red')\n",
        "\n",
        "    G = assets[\"G\"]\n",
        "    device = assets[\"device\"]\n",
        "\n",
        "    z_tensor = torch.from_numpy(st.session_state.z_latent).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        w_latent = G.mapping(z_tensor, None)\n",
        "        w_latent_new = w_latent + \\\n",
        "                       assets['age_w'] * st.session_state.age_strength + \\\n",
        "                       assets['smile_w'] * st.session_state.smile_strength + \\\n",
        "                       assets['gender_w'] * st.session_state.gender_strength\n",
        "        img_tensor = G.synthesis(w_latent_new, noise_mode='const')\n",
        "\n",
        "    return tensor_to_pil(img_tensor)\n",
        "\n",
        "# --- Session State Initialization ---\n",
        "if 'image' not in st.session_state:\n",
        "    if st.session_state.backend_assets:\n",
        "        G = st.session_state.backend_assets[\"G\"]\n",
        "        st.session_state.z_latent = np.random.randn(1, G.z_dim)\n",
        "        st.session_state.age_strength = 0.0\n",
        "        st.session_state.smile_strength = 0.0\n",
        "        st.session_state.gender_strength = 0.0\n",
        "        st.session_state.image = generate_image()\n",
        "\n",
        "# --- Callbacks ---\n",
        "def update_image_callback():\n",
        "    st.session_state.image = generate_image()\n",
        "\n",
        "def random_face_callback():\n",
        "    G = st.session_state.backend_assets[\"G\"]\n",
        "    st.session_state.z_latent = np.random.randn(1, G.z_dim)\n",
        "    # Reset sliders and update image\n",
        "    st.session_state.age_strength = 0.0\n",
        "    st.session_state.smile_strength = 0.0\n",
        "    st.session_state.gender_strength = 0.0\n",
        "    update_image_callback()\n",
        "\n",
        "def reset_sliders_callback():\n",
        "    st.session_state.age_strength = 0.0\n",
        "    st.session_state.smile_strength = 0.0\n",
        "    st.session_state.gender_strength = 0.0\n",
        "    update_image_callback()\n",
        "\n",
        "# --- UI Layout ---\n",
        "st.title(\"ðŸŽ¨ Real-Time Latent Editing Interface\")\n",
        "\n",
        "# Check if backend loaded properly before drawing the main UI\n",
        "if not st.session_state.backend_assets or 'image' not in st.session_state:\n",
        "    st.error(\"Application failed to initialize. Please check the logs.\")\n",
        "    # Add a button to rerun the script if initialization fails\n",
        "    if st.button(\"Retry Initialization\"):\n",
        "        st.caching.clear_cache()\n",
        "        st.experimental_rerun()\n",
        "else:\n",
        "    with st.sidebar:\n",
        "        st.header(\"Controls\")\n",
        "        st.slider(\"Age\", -5.0, 5.0, key=\"age_strength\", on_change=update_image_callback)\n",
        "        st.slider(\"Smile\", -5.0, 5.0, key=\"smile_strength\", on_change=update_image_callback)\n",
        "        st.slider(\"Gender\", -5.0, 5.0, key=\"gender_strength\", on_change=update_image_callback)\n",
        "        st.markdown(\"---\")\n",
        "        col1, col2 = st.columns(2)\n",
        "        with col1:\n",
        "            st.button(\"New Face\", on_click=random_face_callback, use_container_width=True)\n",
        "        with col2:\n",
        "            st.button(\"Reset Sliders\", on_click=reset_sliders_callback, use_container_width=True)\n",
        "        st.download_button(\n",
        "            label=\"ðŸ’¾ Save Image\",\n",
        "            data=image_to_bytes(st.session_state.image),\n",
        "            file_name=\"generated_face.png\",\n",
        "            mime=\"image/png\",\n",
        "            use_container_width=True,\n",
        "        )\n",
        "\n",
        "    st.image(st.session_state.image, caption=\"Generated by the Backend\", use_column_width=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0nhiTlO-KsZ"
      },
      "outputs": [],
      "source": [
        "!curl ipecho.net/plain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install all necessary packages quietly\n",
        "!pip install -q streamlit numpy torch requests Pillow\n",
        "\n",
        "# Step 2: Run the Streamlit app in the background on a fixed port (8501)\n",
        "!streamlit run app.py --server.port 8501 &>/dev/null&\n",
        "\n",
        "# Step 3: Wait a few seconds for the app to start up\n",
        "import time\n",
        "time.sleep(10)\n",
        "\n",
        "# Step 4: Use localtunnel to expose the correct port\n",
        "!npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "M9lJqFl6kCF-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPaZqd5OOJvnIZ5GRJTx/QD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}